{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tonysong0801/Airbnb-Data-Analysis/blob/main/A6\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erekhZZEfGmq"
      },
      "source": [
        "# Assignment 6 - Object recognition in images with deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZSWdU5yfGmr"
      },
      "source": [
        "## Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doGKvr12fGmr"
      },
      "source": [
        "In this assignment you will get to know the main ingredients of deep learning and get to use the GPUs available in the Big Data Lab.\n",
        "\n",
        "You'll learn to use\n",
        "\n",
        " * tensors and automatic differentiation\n",
        " * layered models\n",
        " * p(re)trained networks for image classification.\n",
        "\n",
        "## Check the GPU setup\n",
        "\n",
        "When you are logged in to a lab machine, run ``nvidia-smi`` to see the available card and its memory usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uxK2qSpfGmr"
      },
      "source": [
        "```\n",
        "$ nvidia-smi\n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\n",
        "| 45%   24C    P8    N/A /  75W |   3087MiB /  4038MiB |      0%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                       GPU Memory |\n",
        "|  GPU       PID   Type   Process name                             Usage      |\n",
        "|=============================================================================|\n",
        "|    0      3627      G   /usr/lib/xorg/Xorg                           169MiB |\n",
        "|    0     10843      C   ...d/CMPT/big-data/tmp_py/dlenv/bin/python  2897MiB |\n",
        "+-----------------------------------------------------------------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guR4jU0UfGmr"
      },
      "source": [
        "This shows that the machine has an **NVIDIA GTX 1050 with 4G of RAM**. Also, you can see that it is running a process (pid=10843) that currently takes up close to 3 GB of GPU memory. On our current `blu9402` lab machines you will notice a difference, as they have 8GB of RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxQTaFZbfGms"
      },
      "source": [
        "```\n",
        "$ pstree -ls 10843\n",
        "screen───bash───jupyter-noteboo───python─┬─4*[python]\n",
        "                                         └─26*[{python}]\n",
        "```\n",
        "Inside a terminal window you may use ``who``, ``ps -aux | less``, or ``pstree -ls <PID>`` as above to find out who is using the shared resources. In my case, it turns out that I'm running a jupyter notebook related to process 10843. Halting the notebook frees up the GPU memory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j5XYH7ofGms"
      },
      "source": [
        "## PyTorch setup in the lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2wrLyPgfGms"
      },
      "source": [
        "To build deep learning models in this assignment we are using **[PyTorch](http://pytorch.org)**, a replacement for numpy that provides accelerated computation on the GPU, automatic differentiation, and various utilities to train and deploy neural networks. Its popularity relative to [tensorflow](http://tensorflow.org) has been steadily increasing and it also has a high-level API, the [NN module](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) similar to [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras).\n",
        "\n",
        "In case you have trouble configuring a conda environment that has a CUDA version of pytorch installed, you could use the one that's provided under the prefix  \n",
        "`conda activate /usr/shared/CMPT/big-data/condaenv/gt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2qpLresfGms"
      },
      "source": [
        "### Save disk space in the lab: Use shared downloaded pre-built models\n",
        "To save disk space in your home folder, we recommend that you let pytorch use the pre-built models that we already downloaded for you (about 1.9G):\n",
        "```\n",
        "mkdir -p ~/.cache/torch/hub/checkpoints\n",
        "ln -s /usr/shared/CMPT/big-data/dot_torch_shared/checkpoints/* ~/.cache/torch/hub/checkpoints\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDwIEDwqfGms"
      },
      "source": [
        "## Learn about Pytorch usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTH1mjfZfGms"
      },
      "source": [
        "To familiarize yourself with PyTorch, have a look at the [Examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html) on Tensors, or the NN module, or briefly skim over the [60 min blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZKfA-KhfGmt"
      },
      "source": [
        "# Task 1: Finding rectangles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6esdJ7efGmt"
      },
      "source": [
        "A nice [**blog-post** by Johannes Rieke](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) presents a simple setup from scratch that finds rectangles in a black & white image. In order to play with it, we just have to translate a few calls from Keras to PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWZyTQIJfGmt"
      },
      "source": [
        "To familiarize yourself with using pytorch, have a look at the [Examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html). The following code is preparing our training setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8yedMo_fGmt"
      },
      "outputs": [],
      "source": [
        "# to check GPU memory, uncomment and run the following line\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ka7jAfdfGmt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from IPython.display import display, Markdown\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# Create images with random rectangles and bounding boxes. \n",
        "num_imgs = 50000\n",
        "\n",
        "img_size = 8\n",
        "min_object_size = 1\n",
        "max_object_size = 4\n",
        "num_objects = 1\n",
        "\n",
        "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
        "imgs = np.zeros((num_imgs, img_size, img_size))  # set background to 0\n",
        "\n",
        "for i_img in range(num_imgs):\n",
        "    for i_object in range(num_objects):\n",
        "        w, h = np.random.randint(min_object_size, max_object_size, size=2)\n",
        "        x = np.random.randint(0, img_size - w)\n",
        "        y = np.random.randint(0, img_size - h)\n",
        "        imgs[i_img, x:x+w, y:y+h] = 1.  # set rectangle to 1\n",
        "        bboxes[i_img, i_object] = [x, y, w, h]\n",
        "\n",
        "display(Markdown(f\"The training data consists of ${num_imgs:,}$ example images.  \\n\"\n",
        "                 f\"`imgs` is an array of shape {imgs.shape}, giving an ${img_size}\\\\times{img_size}$ pixel image for each example.  \\n\"\n",
        "                 f\"`bboxes` is an array of shape {bboxes.shape}, giving a $1\\\\times4$ row vector [x, y, w, h] for each rectangle.\"\n",
        "                ))\n",
        "display(Markdown('**Here is an example of the training data:**'))\n",
        "i = 0\n",
        "plt.imshow(imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
        "for bbox in bboxes[i]:\n",
        "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none', lw=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Co9MG6nfGmu"
      },
      "outputs": [],
      "source": [
        "# Reshape and normalize the image data to mean 0 and std 1. \n",
        "X = (imgs.reshape(num_imgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
        "display(Markdown(f\"New shape of `imgs`: {X.shape}, with normalized mean {np.mean(X):.2f} and stdev {np.std(X):.2f}\"))\n",
        "\n",
        "# Normalize x, y, w, h by img_size, so that all values are between 0 and 1.\n",
        "# Important: Do not shift to negative values (e.g. by setting to mean 0), because the IOU calculation needs positive w and h.\n",
        "y = bboxes.reshape(num_imgs, -1) / img_size\n",
        "y.shape, np.mean(y), np.std(y)\n",
        "\n",
        "# Split training and test.\n",
        "i = int(0.8 * num_imgs)\n",
        "train_X = X[:i]\n",
        "test_X = X[i:]\n",
        "train_y = y[:i]\n",
        "test_y = y[i:]\n",
        "test_imgs = imgs[i:]\n",
        "test_bboxes = bboxes[i:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsiRjsSFfGmu"
      },
      "source": [
        "## Task 1a\n",
        "**Construct a Pytorch model that resembles the Keras one in the original blog post**, i.e. have a fully connected, hidden layer with 200 neurons, ReLU nonlinearity and dropout rate of 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEHUPJNAfGmu"
      },
      "outputs": [],
      "source": [
        "#model = ...\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(64, 200)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(200, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MyNet().cuda()\n",
        "\n",
        "learning_rate = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3620U_2KfGmu"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adadelta(model.parameters(),lr=learning_rate)\n",
        "loss_fn = torch.nn.MSELoss(size_average=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT6hGkDSfGmv"
      },
      "outputs": [],
      "source": [
        "inputs = Variable(torch.Tensor(train_X)).cuda()\n",
        "labels = Variable(torch.Tensor(train_y)).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZzqSI_qfGmv"
      },
      "outputs": [],
      "source": [
        "phase = 'train'\n",
        "model.train()\n",
        "do_test_loss = False\n",
        "\n",
        "loss_record = []\n",
        "loss_test_record = []\n",
        "for epoch in range(3000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss_record.append(loss.data.item())\n",
        "\n",
        "    if phase == 'train':\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if do_test_loss:\n",
        "        outputs_test = model(inputs_test)\n",
        "        loss_test = loss_fn(outputs_test, labels_test)\n",
        "        loss_test_record.append(loss_test.data.item())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clzf6CvSfGmv"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_record)\n",
        "if do_test_loss:\n",
        "    plt.plot(loss_test_record)\n",
        "    plt.legend([\"train loss\", \"test loss\"])\n",
        "else:\n",
        "    plt.legend([\"train loss\"])\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqfe-5tfGmv"
      },
      "source": [
        "**Change the model from training to evaluation mode** to improve testing performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOWCOkAvfGmv"
      },
      "outputs": [],
      "source": [
        "phase = 'test'\n",
        "# TODO\n",
        "# ...\n",
        "\n",
        "inputs_test = Variable(torch.Tensor(test_X)).cuda()\n",
        "labels_test = Variable(torch.Tensor(test_y)).cuda()\n",
        "\n",
        "model.train()\n",
        "do_test_loss = True\n",
        "\n",
        "loss_record = []\n",
        "loss_test_record = []\n",
        "for epoch in range(5000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss_record.append(loss.data.item())\n",
        "\n",
        "    if phase == 'test':\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.eval()\n",
        "    \n",
        "    if do_test_loss:\n",
        "        outputs_test = model(inputs_test)\n",
        "        loss_test = loss_fn(outputs_test, labels_test)\n",
        "        loss_test_record.append(loss_test.data.item())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b18LGVpfGmv"
      },
      "outputs": [],
      "source": [
        "# Predict bounding boxes on the test images.\n",
        "pred_y = model(Variable(torch.Tensor(test_X)).cuda())\n",
        "\n",
        "pred_bboxes = pred_y.data * img_size\n",
        "pred_bboxes = pred_bboxes.cpu().numpy().reshape(len(pred_bboxes), num_objects, -1)\n",
        "pred_bboxes.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF6emWY0fGmv"
      },
      "outputs": [],
      "source": [
        "def IOU(bbox1, bbox2):\n",
        "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
        "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
        "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
        "\n",
        "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
        "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
        "    if w_I <= 0 or h_I <= 0:  # no overlap\n",
        "        return 0.\n",
        "    I = w_I * h_I\n",
        "    U = w1 * h1 + w2 * h2 - I\n",
        "    return I / U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMAhNJcFfGmv"
      },
      "outputs": [],
      "source": [
        "# Show a few images and predicted bounding boxes from the test dataset. \n",
        "plt.figure(figsize=(12, 3))\n",
        "for i_subplot in range(1, 5):\n",
        "    plt.subplot(1, 4, i_subplot)\n",
        "    i = np.random.randint(len(test_imgs))\n",
        "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
        "    for pred_bbox, exp_bbox in zip(pred_bboxes[i], test_bboxes[i]):\n",
        "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], ec='r', fc='none'))\n",
        "        plt.annotate('IOU: {:.2f}'.format(IOU(pred_bbox, exp_bbox)), (pred_bbox[0], pred_bbox[1]+pred_bbox[3]+0.2), color='r')\n",
        "# Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
        "summed_IOU = 0.\n",
        "for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 4)):\n",
        "    summed_IOU += IOU(pred_bbox, test_bbox)\n",
        "mean_IOU = summed_IOU / len(pred_bboxes)\n",
        "mean_IOU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBUtwfH1fGmw"
      },
      "source": [
        "## Task 1b:\n",
        "Move the computation that is currently done on the CPU over to the GPU using CUDA and increase the number of epochs.  **Improve the training setup**, possibly also changing model or optimizer, until you **reach a test IOU above 0.9**.\n",
        "\n",
        "You can make the changes that move computation to the GPU directly in the cells above as part of 1a.\n",
        "\n",
        "You may get stuck not achieving test IOU above 0.6. In that case, learn about switching the model to evaluation mode and apply the change above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80gz3DC1fGmw"
      },
      "source": [
        "## Question 1c:\n",
        "Why does `eval` mode above have such a significant effect on test performance? Please give a short answer below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inNTAaZ3fGmw"
      },
      "source": [
        "Because dropout part of the model have distinct behaviors during training and evaluation, it is necessary to specify which part pertains to training and which part pertains to testing. By calling eval(), the Dropout will not be used in testing phase, which will diminish overfitting and improve the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3saden3fGmw"
      },
      "source": [
        "# Task 2: Use a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlBNo_8JfGmw"
      },
      "source": [
        "As mentioned in class, deep learning systems are hardly ever developed from scratch, but usually work by refining existing solutions to similar problems. For the following task, we'll **work through the \n",
        "[Transfer learning tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)**, which also provides a ready-made jupyter notebook.\n",
        "\n",
        " **2.1.** Download the notebook and get it to run in your environment. This also involves downloading the bees and ants dataset.  \n",
        " **2.2.** Perform your own training with the provided setup, fill out the answer to Task 2.2 below.  \n",
        " **2.3.** Change the currently chosen pretrained network (resnet) to a different one. At least try out VGG and one other type and use the \"conv net as fixed feature extractor\" approach, fill out the answer to Task 2.3 below.  \n",
        " **2.4.** Load a picture that you took yourself and classify it with an unmodified pretrained network (e.g. the original VGG network) that can detect one out of 1000 classes. Fill out the answer to Task 2.4 below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYSJaUvfGmw"
      },
      "source": [
        "## Your solution for Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTu-w7OyfGmw"
      },
      "source": [
        "Before you start, get the data from [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip) and extract it into a subfolder `data`. The following import is going to attempt loading the image data from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojmyQdQpfGmw"
      },
      "source": [
        "Initialize much of the source code from the tutorial notebook located at\n",
        "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "using [this module](https://github.com/sfu-db/bigdata-cmpt733/blob/master/Assignments/A6/tfl_tut.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzaZJ81qfGmw"
      },
      "outputs": [],
      "source": [
        "import zipfile as zf \n",
        "files = zf.ZipFile('hymenoptera_data.zip', 'r') \n",
        "files.extractall('data') \n",
        "files.close()\n",
        "from tfl_tut import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWm9SEQdfGmw"
      },
      "source": [
        "Please study the original notebook and then continue to use its functions as imported from the `tfl_tut` model for convenience to minimize source code copy & paste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imf-VKVUfGmx"
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNpaQ2o6fGmx"
      },
      "outputs": [],
      "source": [
        "model_conv = models.inception_v3(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-0riEXWfGmx"
      },
      "source": [
        "### Answer for Task 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MZMgdN8fGmx"
      },
      "outputs": [],
      "source": [
        "# TODO paste and maybe modify relevant code to perform your own training\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "model_conv=model_conv.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "model_conv = train_model(model_conv, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVqjJ9AcfGmx"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdq4llnUfGmx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHH0tNPifGmx"
      },
      "source": [
        "### Answer for Task 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyYwz6W7fGmx"
      },
      "source": [
        "### Hints for this task\n",
        "\n",
        "Focus on the section **Conv net as fixed feature xtractor** of the transfer learning tutorial.\n",
        "First, change the line\n",
        "```\n",
        "model_conv = models.resnet18(pretrained=True)\n",
        "```\n",
        "to load VGG16 instead. Set all its parameters to *not* require gradient computation, as shown in the tutorial.\n",
        "\n",
        "Next, print out the new `model_conv` and identify the last step of the classification. This is not named the same way as the ```fc``` layer for resnet, but it works similarily. The last classification step of the VGG model determines the probabilities for each of the 1000 classes of the dataset. Change this layer to identify only 2 classes to distinguish ants and bees as in the example.\n",
        "\n",
        "To change the structure of some `Sequential` component called ```model_conv.module_name``` and to modify its last layer into a `DifferentLayer` type, you can use this syntax:\n",
        "\n",
        "```\n",
        "nn.Sequential(*list(model_conv.module_name.children())[:-1] +\n",
        "                     [nn.DifferentLayer(...)])\n",
        "```\n",
        "and replace the old `model_conv.module_name` with this differently structured version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvxb74J7fGmx"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "model_conv = torchvision.models.vgg16(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "model_conv.classifier= nn.Sequential(*list(model_conv.classifier.children())[:-1] +\n",
        "                                     [nn.Linear(in_features=4096, out_features=2)])\n",
        "# print(nn.Sequential(*list(model_conv.children())))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.classifier[6].parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "model_ft = train_model(model_conv.cuda(), criterion, optimizer_conv, exp_lr_scheduler,\n",
        "                       num_epochs=25)\n",
        "                       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuEFK5Y2fGmx"
      },
      "source": [
        "### Answer for Task 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6VJaOK7fGmx"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "vgg_16 = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "from PIL import Image\n",
        "image = Image.open(\"fish.jpg\")\n",
        "img_transforms = transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "import requests\n",
        "dataset = requests.get('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json')\n",
        "labels = {int(k): v for k,v in dataset.json().items()}\n",
        "\n",
        "vgg_16.eval()\n",
        "pred = vgg_16(img_transforms(image).unsqueeze(0)).data.numpy().argmax()\n",
        "plt.imshow(image)\n",
        "print(labels[pred])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUpandujfGmy"
      },
      "source": [
        "Please include the picture so we can view it and its class label in the saved notebook. It's OK, if we don't have the actual image file to reproduce the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5x7_y3fGmy"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3JRWcFkfGmy"
      },
      "source": [
        "Your submission should be based on a modified version of [this notebook](https://github.com/sfu-db/bigdata-cmpt733/blob/master/Assignments/A6/A6.ipynb) containing answers to Task 1 and for Task 2, saved with figures including some portions of the transfer learning tutorial notebook in the sections for tasks 2.1 - 2.4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2axJLPRVfGmy"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}